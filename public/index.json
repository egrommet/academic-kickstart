[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Senior Lecturer in Data Journalism. For the last 12 years I\u0026rsquo;ve been working at Cardiff University\u0026rsquo;s School of Journalism, Media and Culture but from June I\u0026rsquo;ll be joining City, University of London.\nI\u0026rsquo;m currently co-director of the MSc in Computational and Data Journalism and a co-founder of the European Data and Computational Journalism Conference. My interests include data journalism, journalism and software development, social media and production journalism.\n","date":-62135596800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":-62135596800,"objectID":"598b63dd58b43bce02403646f240cd3c","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"author","summary":"I\u0026rsquo;m a Senior Lecturer in Data Journalism. For the last 12 years I\u0026rsquo;ve been working at Cardiff University\u0026rsquo;s School of Journalism, Media and Culture but from June I\u0026rsquo;ll be joining City, University of London.\nI\u0026rsquo;m currently co-director of the MSc in Computational and Data Journalism and a co-founder of the European Data and Computational Journalism Conference. My interests include data journalism, journalism and software development, social media and production journalism.","tags":null,"title":"Glyn Mottershead","type":"author"},{"authors":null,"categories":null,"content":" A place for data journalism tutorials using different tools, but coding will be primarily in R, using the RStudio IDE. You can have a play with the cloud version of RStudio.\nGetting started with the Tidyverse Looking at police data using a Tidyverse pivot table, the is the first of the Tidyverse tutorials.\nMapping with ggplot Mapping in R and ggplot - a tutorial rendered from an R markdown file.\nThe second post goes further and looks at making things more interesting.\n","date":1553089980,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1553089980,"objectID":"c3224f3a64174f08aaf31e1f1d16ffd3","permalink":"/tutorial/","publishdate":"2019-03-20T13:53:00Z","relpermalink":"/tutorial/","section":"tutorial","summary":"A place for data journalism tutorials using different tools, but coding will be primarily in R, using the RStudio IDE. You can have a play with the cloud version of RStudio.\nGetting started with the Tidyverse Looking at police data using a Tidyverse pivot table, the is the first of the Tidyverse tutorials.\nMapping with ggplot Mapping in R and ggplot - a tutorial rendered from an R markdown file.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":["R"],"content":" Looking at police data using Tidyverse tools The Tidyverse tools are one of my favourite things and made my R coding experience much simpler. Most of the things I\u0026rsquo;ll be posting will use them in one way or another.\nWe\u0026rsquo;re going to find out what happens when someone reports a stolen bicycle - a friend seems to believe the police do nothing, so let\u0026rsquo;s investigate what actually happens.\nDownloading the data I\u0026rsquo;ve downloaded a month\u0026rsquo;s worth of data for South Wales Police from data.police.uk.\nI\u0026rsquo;m going to do the R equivalent of a pivot table, grouping information together and then analysing it.\nGetting started Open RStudio and go to FILE \u0026gt; NEW PROJECT then create it in a new folder. This will give you a chance to choose where you want it to go. I tend to have a project folder and then each individual project is stored inside that.\nWe\u0026rsquo;ll leave RStudio for a second and head into the computer\u0026rsquo;s file manager to find the folder we just created in RStudio. Inside that I always create a folder called data, I\u0026rsquo;ll then put my downloaded data inside it.\nCreate a script Back inside RStudio, I\u0026rsquo;m going to go to File menu \u0026gt; New File \u0026gt; R Script. I\u0026rsquo;m doing this as it allows me to repeat this analysis when I want to. You\u0026rsquo;ll want to give it a name that makes sense and then save it. R files have a .R file extension at the end.\nInstalling your first R Package A package is a tool, or group of tools, that someone has created to cut out some of the repetition in R. We\u0026rsquo;ll be using the tidyverse by Hadley Wickham. These are a set of really helpful data tools that also make R programming easier to read.\nWe can install packages by going to the Tools menu \u0026gt; Install Packages and type the name into the pop-up box.\nOr I can write the command (known as a function) into my script, highlight my line of code and hit the RUN button at the top of the script. The other way to do it is a line of code - install.packages(\u0026quot;tidyverse\u0026quot;).\nI then need to load it in my script. This is done by the library() function. Again, run the line of code.\nlibrary(tidyverse)  Loading data Even though we have the data set in our R project folder we need to load it up.\nI\u0026rsquo;m going to use read_csv() from readr to read a CSV file into our RStudio environment.\nIf you ever want to know how what a package or function does then type ?function_name in the console eg ?read_csv, and you\u0026rsquo;ll get a help message.\nOur function wrapping works like this read_csv().\nWe need to put the address (** aka file path**) for our CSV inside quote marks inside the inner pair of brackets. It will look like this - read_csv(\u0026quot;file_path\u0026quot;).\nThe full syntax (not the working code I want to use) looks like this: data_frame_name \u0026lt;- read_csv(\u0026quot;file_path\u0026quot;)\nThe \u0026lt;- is an assignment symbol, I tend to think of it as a hosepipe which allows the function to pour into the name we give our data frame.\nMy file path in the example below says ./ inside the folder we are currently in is a folder called data and / inside that is a csv file.\ncrimes \u0026lt;- read_csv(\u0026quot;./data/2019-01-south-wales-street.csv\u0026quot;)  ## Parsed with column specification: ## cols( ## `Crime ID` = col_character(), ## Month = col_character(), ## `Reported by` = col_character(), ## `Falls within` = col_character(), ## Longitude = col_double(), ## Latitude = col_double(), ## Location = col_character(), ## `LSOA code` = col_character(), ## `LSOA name` = col_character(), ## `Crime type` = col_character(), ## `Last outcome category` = col_character(), ## Context = col_logical() ## )  The output for this will show us what types of columns we have - in this case col_character() means text and col_double() means a number format. All good so far.\nFiltering with dplyr I want to filter the information so it just includes bicycle thefts. I\u0026rsquo;m going to use some dplyr package tricks to do this.\nI\u0026rsquo;m going to store it in a new data frame called bikes. We\u0026rsquo;ll be using the crimes data frame we made earlier and then filtering it, before assigning the new information to bikes. So bikes \u0026lt;- crimes.\nThe next bit of the code is %\u0026gt;% a pipe - essentially you can say and then to yourself when you write it. Our next function is a filter(), it does a similar job to the Excel function.\nMost functions will need you to say which data frame you are working on and then what you want to do. So we could say filter(data_frame, column_name == \u0026quot;text we want back\u0026quot;).\nBut dplyr knows we are working on the data frame before the first %\u0026gt;%.\nbikes \u0026lt;- crimes %\u0026gt;% filter(`Crime type` == \u0026quot;Bicycle theft\u0026quot;) # If I use the dataframe name on its own and run that line I can see what is in it bikes  ## # A tibble: 126 x 12 ## `Crime ID` Month `Reported by` `Falls within` Longitude Latitude ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 0fad86217… 2019… South Wales … South Wales P… -3.54 51.5 ## 2 5ea5f1bb3… 2019… South Wales … South Wales P… -3.19 51.5 ## 3 ab5c7a5c7… 2019… South Wales … South Wales P… -3.12 51.5 ## 4 44e702929… 2019… South Wales … South Wales P… -3.23 51.5 ## 5 979ee8076… 2019… South Wales … South Wales P… -3.09 51.5 ## 6 34490753e… 2019… South Wales … South Wales P… -3.12 51.5 ## 7 59c9863ef… 2019… South Wales … South Wales P… -3.12 51.5 ## 8 7029c550b… 2019… South Wales … South Wales P… -3.22 51.5 ## 9 f8f063bad… 2019… South Wales … South Wales P… -3.22 51.5 ## 10 cdd878455… 2019… South Wales … South Wales P… -3.20 51.5 ## # … with 116 more rows, and 6 more variables: Location \u0026lt;chr\u0026gt;, `LSOA ## # code` \u0026lt;chr\u0026gt;, `LSOA name` \u0026lt;chr\u0026gt;, `Crime type` \u0026lt;chr\u0026gt;, `Last outcome ## # category` \u0026lt;chr\u0026gt;, Context \u0026lt;lgl\u0026gt;  The output will show us how many rows (observations) and columns (variables) there are - in this case a 126 x 12. We can start adding some more analysis now.\nFiltering and renaming for speed You may notice when we printed the data frame that some of the column names have backticks around them. That\u0026rsquo;s because they\u0026rsquo;ve got a space between the words and R is trying to help out. So, if we wanted to work with our Crime ID column we\u0026rsquo;d actually have to type Crime ID with the back ticks.\nI\u0026rsquo;m too lazy for things like that. I\u0026rsquo;d rather change the name so I can write something easier. We\u0026rsquo;re going to use rename() to do it. We will tell rename() that our \u0026quot;new column name\u0026quot; = \u0026quot;old column name\u0026quot; - but make sure you put the quote marks around the names when you do this.\nSo, I\u0026rsquo;m going to tweak my code from before. I\u0026rsquo;m going to change the name of the Last outcome category and the Crime type columns. I\u0026rsquo;m going to change them to Outcome and Crimes - much shorter to type.\nSo, we\u0026rsquo;re telling dplyr to take the crimes data frame and then rename by assigning the new name to the old name. (The comma means we can do more than one rename at once, to do more just add a comma and then say \u0026ldquo;new\u0026rdquo; = \u0026ldquo;old\u0026rdquo; again.) And then filter our newly-named Crimes column and look for the text inside the quote marks.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;)  Doing a pivot table in dplyr syntax Now we can do some digging. I want to know what the police have done about the reported bike thefts.\nWe can group things together by our Outcome category and count how often each of the outcomes happen. So, we tell group_by() which column we want to work on. And then we tell it what to count - in this case it creates a column called outcome_count and uses the n() function to count the Outcome.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;) %\u0026gt;% group_by(Outcome) %\u0026gt;% summarise(outcome_count = n()) # Again I'll type the dataframe name here and run this line to see what we have got. bikes  ## # A tibble: 4 x 2 ## Outcome outcome_count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Awaiting court outcome 1 ## 2 Investigation complete; no suspect identified 88 ## 3 Unable to prosecute suspect 1 ## 4 Under investigation 36  We\u0026rsquo;ve got a pivot table - it has grouped the Outcome column and counted how often each one occurs.\nTLDR: \u0026ldquo;Investigation complete; no suspect identified\u0026rdquo; = \u0026ldquo;No suspect identified\u0026rdquo; We\u0026rsquo;re going to change an element by using the mutate() function - we\u0026rsquo;re going to recode() one of our Outcome categories to shorten it. The code would look like this - mutate(Outcome = recode(Outcome, \u0026quot;Investigation complete; no suspect identified\u0026quot; = \u0026quot;No suspect identified\u0026quot;))\nBut I\u0026rsquo;d like to order it in descending order - so we can do one more and then %\u0026gt;% at the end of our code. We will arrange() our outcome_count in desc() - descending order. You must put desc() inside the arrange() function. And check your brackets.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;) %\u0026gt;% mutate(Outcome = recode(Outcome, \u0026quot;Investigation complete; no suspect identified\u0026quot; = \u0026quot;No suspect identified\u0026quot;)) %\u0026gt;% group_by(Outcome) %\u0026gt;% summarise(outcome_count = n()) %\u0026gt;% arrange(desc(outcome_count)) # Again I'll type the dataframe name here and run this line to see what we have got. bikes  ## # A tibble: 4 x 2 ## Outcome outcome_count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 No suspect identified 88 ## 2 Under investigation 36 ## 3 Awaiting court outcome 1 ## 4 Unable to prosecute suspect 1  And we\u0026rsquo;ve got it. A pivot table that tells us what has happened to the bike crimes in the original data set and has put them in descending order.\n","date":1554768000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554768000,"objectID":"7eb41758d7fa18678feddcba2ca94522","permalink":"/tutorial/pivot-tables-in-r-looking-at-police-data/","publishdate":"2019-04-09T00:00:00Z","relpermalink":"/tutorial/pivot-tables-in-r-looking-at-police-data/","section":"tutorial","summary":"Looking at police data using Tidyverse tools The Tidyverse tools are one of my favourite things and made my R coding experience much simpler. Most of the things I\u0026rsquo;ll be posting will use them in one way or another.\nWe\u0026rsquo;re going to find out what happens when someone reports a stolen bicycle - a friend seems to believe the police do nothing, so let\u0026rsquo;s investigate what actually happens.\nDownloading the data I\u0026rsquo;ve downloaded a month\u0026rsquo;s worth of data for South Wales Police from data.","tags":["Police"],"title":"Pivot tables in R - looking at police data","type":"tutorial"},{"authors":["Glyn Mottershead"],"categories":null,"content":"","date":1554381000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554381000,"objectID":"74e6a49cd95b6782e0b5665eb8dd9b8d","permalink":"/talk/digital-city/","publishdate":"2019-03-07T00:00:00Z","relpermalink":"/talk/digital-city/","section":"talk","summary":"Data, algorithms, chatbots and computational journalism impacting on newsrooms, are you ready?","tags":[],"title":"Data and Computational Journalism","type":"talk"},{"authors":null,"categories":["R","Data Visualisation","Mapping"],"content":" Mapping in R - using ggplot2, part two In the previous tutorial we looked at getting data from web APIs in JSON and GeoJSON formats to create a simple map.\nThis time we\u0026rsquo;ll be adapting code from Timo Grossenbacher to make our map more attractive.\nGetting started We need to reload the data again, so we\u0026rsquo;re going to get a different number of signatures than last time.\nI\u0026rsquo;m going to run this in one block and assume you can follow along, if not go back to the previous post.\n# Load the mapping file library(geojsonio)  ## ## Attaching package: 'geojsonio'  ## The following object is masked from 'package:base': ## ## pretty  url \u0026lt;- \u0026quot;https://opendata.arcgis.com/datasets/5ce27b980ffb43c39b012c2ebeab92c0_2.geojson\u0026quot; uk_map \u0026lt;- geojson_read(url, what = \u0026quot;sp\u0026quot;) # Convert to a ggplot-friendly format library(ggplot2)  ## Registered S3 methods overwritten by 'ggplot2': ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang  ## Registered S3 method overwritten by 'dplyr': ## method from ## print.location geojsonio  fort_uk_map \u0026lt;- fortify(uk_map, region = \u0026quot;pcon17cd\u0026quot;) # Get the data from the petition site library(jsonlite)  ## ## Attaching package: 'jsonlite'  ## The following object is masked from 'package:geojsonio': ## ## validate  json_data \u0026lt;- fromJSON(\u0026quot;https://petition.parliament.uk/petitions/241584.json\u0026quot;, flatten = FALSE) # Turn it into a datafram sign_data \u0026lt;- json_data$data$attributes$signatures_by_constituency # Store the mumber of signatures for later total_sig \u0026lt;- sum(sign_data$signature_count) # Join the two datasets together library(dplyr)  ## ## Attaching package: 'dplyr'  ## The following objects are masked from 'package:stats': ## ## filter, lag  ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union  full_uk_map \u0026lt;- left_join(fort_uk_map, sign_data, by = c(\u0026quot;id\u0026quot; = \u0026quot;ons_code\u0026quot;))  One of the things I really liked about Timo\u0026rsquo;s post is that he divided the vote count into bands (buckets) to help make it easier to see what is going on. I like the previous map but it isn\u0026rsquo;t massively easy to read. I\u0026rsquo;m not going to go into a lot on data viz theory here, but I strongly recommend you have a look at the work of Alberto Cairo.\nBlue is the colour What I didn\u0026rsquo;t explain last time is that the default colour of our last map was blue. This time I want it to be intentional and echo the blue background of the European flag, so the deeper blues signify more Europhile constituencies.\nThere is a problem with maps - they often just show where the population is biggest, we could go int detail about constituencies to scale it more effectively but it gives us an interesting starting point. \u0026ldquo;The Office for National Statistics gives the median total parliamentary electorate across constituencies of about 72,400 in England, 69,000 in Scotland, 66,800 in Northern Ireland and 56,800 in Wales.\u0026rdquo; Source\nI like to use colorbrewer when I\u0026rsquo;m working with maps, you can have a much more nuanced colour range than using R\u0026rsquo;s default (yes it can be done in R default, but I\u0026rsquo;ve personally found it to be more fiddly). And, it is also a function that is built into ggplot2 scale_fill_brewer() - which will make our life easier.\nBut this gives us a bit of a problem as our colorbrewer function call only allows us nine shades of blue. If we tried using it as it stands we\u0026rsquo;d throw an error message similar to this 1: In brewer.pal(n, pal): n too large, allowed maximum for palette....\nSo, we\u0026rsquo;ll chop our vote range into blocks as per Timo\u0026rsquo;s post, create labels that allow us to explain the blocks, colour it according to our new range and then add a them to make it look good. His post goes into a lot more detail (obviously!).\nThere\u0026rsquo;s a whole in my bucket We\u0026rsquo;ll start off by declaring how many colour buckets we want to use, we\u0026rsquo;ll store it as a variable.\nno_classes \u0026lt;- 9  The next thing to do is then chop our voter range into sections (we want 9), we\u0026rsquo;ll be using quantiles() to do that. We\u0026rsquo;ll give our function the numbers in the signature count column of our full_uk_map. The probs argument takes the number of classes and then calculates the bands for us.\nquantiles \u0026lt;- quantile(full_uk_map$signature_count, probs = seq(0, 1, length.out = no_classes + 1))  Next we create an empty vector for the labels for our map legend, essentially an empty box to store the next stage in.\nThe second part uses a for loop to run through our number range and create the labels. We\u0026rsquo;ll round to the nearest whole number (as you can\u0026rsquo;t get part of a vote, but our quantiles calculation would give that).\nlabels \u0026lt;- c() for(band in 1:length(quantiles)){ labels \u0026lt;- c(labels, paste0(round(quantiles[band]), \u0026quot; – \u0026quot;, round(quantiles[band + 1]))) } labels  ## [1] \u0026quot;1986 – 3917\u0026quot; \u0026quot;3917 – 4607\u0026quot; \u0026quot;4607 – 5345\u0026quot; \u0026quot;5345 – 6543\u0026quot; ## [5] \u0026quot;6543 – 7646\u0026quot; \u0026quot;7646 – 9103\u0026quot; \u0026quot;9103 – 9714\u0026quot; \u0026quot;9714 – 11188\u0026quot; ## [9] \u0026quot;11188 – 37122\u0026quot; \u0026quot;37122 – NA\u0026quot;  And we\u0026rsquo;ve got a problem - the code gives us a final band that looks like \u0026ldquo;36901 - NA\u0026rdquo;, anything over our maximum. We can easily get rid of that as obviously we are limited at the maximum.\nlabels \u0026lt;- labels[1:length(labels)-1] labels  ## [1] \u0026quot;1986 – 3917\u0026quot; \u0026quot;3917 – 4607\u0026quot; \u0026quot;4607 – 5345\u0026quot; \u0026quot;5345 – 6543\u0026quot; ## [5] \u0026quot;6543 – 7646\u0026quot; \u0026quot;7646 – 9103\u0026quot; \u0026quot;9103 – 9714\u0026quot; \u0026quot;9714 – 11188\u0026quot; ## [9] \u0026quot;11188 – 37122\u0026quot;  Next, we need to add our new number range to the map. We\u0026rsquo;ll do this with the cut() function, to turn our number range into a factor.\nfull_uk_map$quantiles \u0026lt;- cut(full_uk_map$signature_count, breaks = quantiles, labels = labels, include.lowest = T)  Making the map Now we can make our map. This time we\u0026rsquo;ll use our new quantiles column as the fill. We\u0026rsquo;ll use scale_fill_brewer() to give us our \u0026lsquo;European\u0026rsquo; blue colour range and put the legend at the bottom. we\u0026rsquo;ll use the labs() element of ggplot2 to give us a headline, captions and attribution.\nby_quantile \u0026lt;- ggplot() + geom_polygon(data = full_uk_map, aes(x = long, y = lat, group = group, fill = quantiles)) + geom_path(color = \u0026quot;black\u0026quot;, size = 0.1) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Blues\u0026quot;, guide = \u0026quot;legend\u0026quot;, name = \u0026quot;Signature\\nCount\u0026quot;, labels = labels) + # I've commmented out theme_void() so you can see what the built-in themes do # theme_void() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;Signatories of the Revoke Article 50 Petition\u0026quot;, subtitle = \u0026quot;Let's investigate where the signatures come from\u0026quot;, caption = \u0026quot;Geometries: ONS Open Geography Portal; Data: UK Parliament and Government\u0026quot;) by_quantile  The next thing we can try is adding a theme. I\u0026rsquo;m just going to use the one from Timo\u0026rsquo;s post but change the font family. It is being stored as a function, so we can call it easily.\ntheme_map \u0026lt;- function(...) { theme_minimal() + theme( text = element_text(family = \u0026quot;Helvetica\u0026quot;, color = \u0026quot;#22211d\u0026quot;), axis.line = element_blank(), axis.text.x = element_blank(), axis.text.y = element_blank(), axis.ticks = element_blank(), axis.title.x = element_blank(), axis.title.y = element_blank(), # panel.grid.minor = element_line(color = \u0026quot;#ebebe5\u0026quot;, size = 0.2), panel.grid.major = element_line(color = \u0026quot;#ebebe5\u0026quot;, size = 0.2), panel.grid.minor = element_blank(), plot.background = element_rect(fill = \u0026quot;#f5f5f2\u0026quot;, color = NA), panel.background = element_rect(fill = \u0026quot;#f5f5f2\u0026quot;, color = NA), legend.background = element_rect(fill = \u0026quot;#f5f5f2\u0026quot;, color = NA), panel.border = element_blank(), ... ) }  And this time we\u0026rsquo;ll use the theme as part of our final version. We\u0026rsquo;re also going to pull a little trick in the title - remember we saved the number of votes in total earlier on as total_sig? We can use the paste0() function to put the number in the text, with the added advantage that the number will automatically update from the API when we run all the code again.\nby_quantile2 \u0026lt;- ggplot() + geom_polygon(data = full_uk_map, aes(x = long, y = lat, group = group, fill = quantiles)) + geom_path(color = \u0026quot;black\u0026quot;, size = 0.1) + scale_fill_brewer(type = \u0026quot;qual\u0026quot;, palette = \u0026quot;Blues\u0026quot;, guide = \u0026quot;legend\u0026quot;, name = \u0026quot;Signature\\nCount\u0026quot;, labels = labels) + theme_void() + coord_equal() + theme_map() + theme(legend.position = \u0026quot;bottom\u0026quot;) + labs(x = NULL, y = NULL, title = \u0026quot;Signatories of the Revoke Article 50 Petition\u0026quot;, subtitle = paste0(\u0026quot;Let's investigate where the \u0026quot;, format(total_sig, big.mark = \u0026quot;,\u0026quot;), \u0026quot; signatures come from\u0026quot;), caption = \u0026quot;Geometries: ONS Open Geography Portal; Data: UK Parliament and Government\u0026quot;) by_quantile2  At some stage I\u0026rsquo;m going to play around with the background to declare the plot size (I\u0026rsquo;m guessing that is why it is breaking the bounds here - but the version at the top is exported from the code run in RStudio), I\u0026rsquo;ll post when I do but for now I\u0026rsquo;m going to stop here.\nRemember, Timo goes on much further to make a really beautiful map.\n","date":1554076800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554076800,"objectID":"eada26b3a8496f1d533e96d8d242253d","permalink":"/tutorial/mapping-the-article-50-petition-in-r-and-ggplot-part-2/","publishdate":"2019-04-01T00:00:00Z","relpermalink":"/tutorial/mapping-the-article-50-petition-in-r-and-ggplot-part-2/","section":"tutorial","summary":"Mapping in R - using ggplot2, part two In the previous tutorial we looked at getting data from web APIs in JSON and GeoJSON formats to create a simple map.\nThis time we\u0026rsquo;ll be adapting code from Timo Grossenbacher to make our map more attractive.\nGetting started We need to reload the data again, so we\u0026rsquo;re going to get a different number of signatures than last time.\nI\u0026rsquo;m going to run this in one block and assume you can follow along, if not go back to the previous post.","tags":["ggplot2","json","map"],"title":"Mapping the article 50 petition in R and ggplot - part 2","type":"tutorial"},{"authors":null,"categories":["R","Mapping","Data Visualisation"],"content":" Mapping in R - using ggplot2 The Revoke Article 50 petition got my colleague and all-round codemeister Dr Martin Chorley and I talking.\nWe started thinking about ways that we could see what the patterns for people signing were like.\nIt was well into the millions when I started playing with ways of visualising where people who voted were located. The site can map all of the signatures (5,962,824 at the time of writing), but it also has an option to get the data in a machine-friendly json format.\nAccording to the site:\nThe data shows the number of people who have signed the petition by country as well as in the constituency of each Member of Parliament. This data is available for all petitions on the site. It is not a list of people who have signed the petition. The only name that is shared on the site is that of the petition creator.\nGetting started You\u0026rsquo;ll need to install the following packages: install.packages(c(\u0026quot;geojsonio\u0026quot;, \u0026quot;ggplot2\u0026quot;, \u0026quot;dplyr\u0026quot;, \u0026quot;jsonlite\u0026quot;))\nGetting the map shapes We\u0026rsquo;re going to use geojsonio first to get a mapping file from the ONS Open Geography Portal. It has a great repository of mapping files. We\u0026rsquo;ll be using the parliamentary wards file. Go to the site and use the menu bar to: Boundaries \u0026gt; Electoral Boundaries \u0026gt; Westminster Parliamentary Constituencies \u0026gt; 2017 Boundaries\nYou can download the file in a variety of formats, but we\u0026rsquo;re going to use the API to import it directly in GeoJSON format.\nNB At the time of writing there appeared to be a glitch in the site, I actually found the right map home page via a search engine.\nFirst we use the library() function to call geojsonio to handle the file, we\u0026rsquo;ll store the URL as a variable and then read it in to our working environment. The \u0026lsquo;what\u0026rsquo; argument uses \u0026ldquo;sp\u0026rdquo; - spacial class for a mapping file.\nlibrary(geojsonio)  ## ## Attaching package: 'geojsonio'  ## The following object is masked from 'package:base': ## ## pretty  url \u0026lt;- \u0026quot;https://opendata.arcgis.com/datasets/5ce27b980ffb43c39b012c2ebeab92c0_2.geojson\u0026quot; uk_map \u0026lt;- geojson_read(url, what = \u0026quot;sp\u0026quot;)  We then need to turn it from the form it is in to something we can map more easily in ggplot2, so we\u0026rsquo;ll call the library here and use the fortify() function.\nBy having a look in side the uk_map dataframe we can see our code names for the constituencies are stored in pcon17cd, so we\u0026rsquo;ll add that as our region.\nlibrary(ggplot2)  ## Registered S3 methods overwritten by 'ggplot2': ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang  ## Registered S3 method overwritten by 'dplyr': ## method from ## print.location geojsonio  fort_uk_map \u0026lt;- fortify(uk_map, region = \u0026quot;pcon17cd\u0026quot;)  Getting the data for our map We\u0026rsquo;re now going to read in the data from the Parliament Petitions site. We\u0026rsquo;ll use jsonlite to do that.\nlibrary(jsonlite)  ## ## Attaching package: 'jsonlite'  ## The following object is masked from 'package:geojsonio': ## ## validate  json_data \u0026lt;- fromJSON(\u0026quot;https://petition.parliament.uk/petitions/241584.json\u0026quot;, flatten = FALSE)  The next thing we need to do is get the data out of the json file we just imported. If you click on the json_data object in the Environment pane, you\u0026rsquo;ll see it is a list of two - double click to open it up and we cab view the file. Inside the json-data structure we can see data has a list of three objects inside it, opening that shows us attributes is where the interesting things are happening.\nThere\u0026rsquo;s a lot going on but there are two things that interest me for mapping - signatures_by_constituency and signatures_by_country (this second one is for a later date).\nOpening the signatures_by_country list shows it has the following elements name, ons_code, mp, signature_count for each of the 650 constituencies in the file. The ons_code will come in useful later when we want to merge our map and data together.\nWe can move through the levels of our json_data object in this fashion name$parent_element$child_element.\nSo in our case: sign_data \u0026lt;- json_data$data$attributes$signatures_by_constituency\nWe\u0026rsquo;ll store that in a dataframe and while we\u0026rsquo;re at it we\u0026rsquo;ll calculate how many signatures there were at the time of running the code, I\u0026rsquo;ll do this as a dataframe as it will be useful in the second tutorial.\nsign_data \u0026lt;- json_data$data$attributes$signatures_by_constituency total_sig \u0026lt;- sum(sign_data$signature_count) total_sig  ## [1] 5721697  Joining the data sets This is where dplyr comes into its own as a data-wrangling toolkit. We\u0026rsquo;ll call the library and then use a left_join() to merge them together into a new dataframe called full_uk_map. There\u0026rsquo;s an explanation of join types on the tidyverse site.\nTo do the join we have to tell the function where our common columns are in the \u0026lsquo;by\u0026rsquo; element \u0026ndash; left_join(dataset1, dataset2, by = c(\u0026quot;a_column\u0026quot; = \u0026quot;the_equivalent_column\u0026quot;))\nlibrary(dplyr)  ## ## Attaching package: 'dplyr'  ## The following objects are masked from 'package:stats': ## ## filter, lag  ## The following objects are masked from 'package:base': ## ## intersect, setdiff, setequal, union  full_uk_map \u0026lt;- left_join(fort_uk_map, sign_data, by = c(\u0026quot;id\u0026quot; = \u0026quot;ons_code\u0026quot;))  Basic ggplot2 map We\u0026rsquo;ll start off with a simple map that shows which constituency people are signing from, so we need to load ggplot2 as a function. I\u0026rsquo;ll break down the structure below for what we are doing here.\n# Call ggplot here as a function and use the '+' symbol to denote 'and then' ggplot() + # We'll use geom_polygon() and tell it where the data is, what our aesthetics are and what to fill how to create it as a choropleth map. geom_polygon(data = full_uk_map, aes(x = long, y = lat, group = group, fill = signature_count)) + # We'll put a white stroke on the constituency boundaries geom_path(color = \u0026quot;white\u0026quot;) + # Get rid of the background theme_void() + # And finally let's use coord_equal to ensure the x and y scales are the same. coord_equal()  Figure 1: A simple choropleth map.\n Next steps I\u0026rsquo;ve also been playing with a great post from Timo Grossenbaher on how to make beatiful thematic maps with ggplot2 to create something a bit more effective.\nNow pop along to stage two of this tutorial which goes further and looks at making things more interesting.\nMore analysis Andy Dickinson from Manchester Met has done a Pandas (Python) look at the article 50 and knife crime petitions.\n","date":1553817600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553817600,"objectID":"ebe7d49627ddf2e04464983e286bdaf5","permalink":"/tutorial/mapping-the-article-50-petition-in-r-and-ggplot/","publishdate":"2019-03-29T00:00:00Z","relpermalink":"/tutorial/mapping-the-article-50-petition-in-r-and-ggplot/","section":"tutorial","summary":"Mapping in R - using ggplot2 The Revoke Article 50 petition got my colleague and all-round codemeister Dr Martin Chorley and I talking.\nWe started thinking about ways that we could see what the patterns for people signing were like.\nIt was well into the millions when I started playing with ways of visualising where people who voted were located. The site can map all of the signatures (5,962,824 at the time of writing), but it also has an option to get the data in a machine-friendly json format.","tags":["json","map","ggplot2"],"title":"Mapping the article 50 petition in R and ggplot","type":"tutorial"},{"authors":["Glyn Mottershead","Martin Chorley"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"2d93fe7431ea20f085cd80d300c6ac11","permalink":"/publication/nieman-predictions-2019/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/nieman-predictions-2019/","section":"publication","summary":"Every time a free tool goes freemium or a platform closes because it can’t make money, we’ve got an issue. What’s going to happen to the stories that tool feeds?","tags":["Journalism","Data Journalism","News Writing","Feature Writing","Journalism Education"],"title":"When a tech company pulls the plug on your story","type":"publication"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1535756400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535756400,"objectID":"79f483fecb548075e055a280902e02cf","permalink":"/publication/are-you-talking-book-chapter/","publishdate":"2018-09-01T00:00:00+01:00","relpermalink":"/publication/are-you-talking-book-chapter/","section":"publication","summary":"This preliminary research aims to show how computational analysis allows us to investigate the means by which news stories are spread.","tags":["Twitter","Data Analysis","Social Media","Journalism Studies"],"title":"Are You Talking To Me?: An Analysis of Journalism Conversation on Social Media","type":"publication"},{"authors":["Jonathan Cable","Glyn Mottershead"],"categories":null,"content":"","date":1520553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520553600,"objectID":"924a674ac01dfa2e7edfd9e659b9c6f8","permalink":"/publication/can-i-click-it/","publishdate":"2018-03-09T00:00:00Z","relpermalink":"/publication/can-i-click-it/","section":"publication","summary":"Does the sports media just use clickbait to generate traffic, and is this is reducing the quality of football journalism?","tags":["Sport Journalism","Data Analysis","Social Media","Journalism Studies"],"title":"'Can I click it? Yes you can': Football journalism, Twitter and clickbait","type":"publication"},{"authors":["Glyn Mottershead","Martin Chorley"],"categories":null,"content":"","date":1504306800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504306800,"objectID":"dc6b0ce608ed2e73e6745eb115fa6759","permalink":"/talk/gap_foj/","publishdate":"2017-09-02T00:00:00+01:00","relpermalink":"/talk/gap_foj/","section":"talk","summary":"Fake news, open data, algorithms and news room experimentation mean there's a need to train journalists beyond the basics.","tags":["Journalism Education"],"title":"Recoding journalism  education: mind the  skills gap, please!","type":"talk"},{"authors":["Glyn Mottershead"],"categories":null,"content":"","date":1499814000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499814000,"objectID":"55e70be41ee4b6555ccc370b7430e92d","permalink":"/talk/visualising-on-a-deadline/","publishdate":"2017-07-12T00:00:00+01:00","relpermalink":"/talk/visualising-on-a-deadline/","section":"talk","summary":"How do you deal with statistical uncertainty when you're up against the clock?","tags":["Journalism Education","Computational Journalism","Data Journalism","Data Visualisation"],"title":"The First Commit of History: Visualising News on a Deadline","type":"talk"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1499295600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499295600,"objectID":"eac104f5060d19d1aedb58c42c608331","permalink":"/talk/now-we-are-three/","publishdate":"2017-07-06T00:00:00+01:00","relpermalink":"/talk/now-we-are-three/","section":"talk","summary":"Key findings from the process of designing a new joint-honours Master’s degree combining Computer Science and Journalism.","tags":["Journalism Education","Computational Journalism","Data Journalism"],"title":"Now we are three: A perspective on Computational and Data Journalism Education","type":"talk"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1475362800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475362800,"objectID":"900514d6a949783fe95035d5f94e4e80","permalink":"/publication/are-you-talking-to-me/","publishdate":"2016-10-02T00:00:00+01:00","relpermalink":"/publication/are-you-talking-to-me/","section":"publication","summary":"This preliminary research aims to show how computational analysis allows us to investigate the means by which news stories are spread.","tags":["Twitter","Data Analysis","Social Media","Journalism Studies"],"title":"Are You Talking To Me?: An Analysis of Journalism Conversation on Social Media","type":"publication"},{"authors":["Tim Holmes","Glyn Mottershead"],"categories":null,"content":"","date":1440716400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440716400,"objectID":"e4680d23b8ea482b608fd38abc270126","permalink":"/publication/subbing-book/","publishdate":"2015-08-28T00:00:00+01:00","relpermalink":"/publication/subbing-book/","section":"publication","summary":"Subediting and Production for Journalists is an introduction to the skills of subediting for newspapers, magazines and websites.","tags":["Subediting","Newspaper Design","Magazine Design"],"title":"Subediting and production for journalists","type":"publication"},{"authors":["Tim Holmes","Sarah Hadwin","Glyn Mottershead"],"categories":null,"content":"","date":1341097200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341097200,"objectID":"23a5953b27b1b69ea603f790f046b6e0","permalink":"/publication/21st-century-journalism-handbook/","publishdate":"2012-07-01T00:00:00+01:00","relpermalink":"/publication/21st-century-journalism-handbook/","section":"publication","summary":"The 21st Century Journalism Handbook is a comprehensive guide to the core principles and practices essential to the modern journalist.","tags":["Journalism","Data Journalism","News Writing","Feature Writing","Journalism Education"],"title":"21st Century Journalism Handbook","type":"publication"},{"authors":["Terry King","Emma Duke-Williams","Glyn Mottershead"],"categories":null,"content":"","date":1249081200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1249081200,"objectID":"b26373d526fa394ae13c765109006d4e","permalink":"/publication/learning-and-knowledge-building/","publishdate":"2009-08-01T00:00:00+01:00","relpermalink":"/publication/learning-and-knowledge-building/","section":"publication","summary":"This paper seeks to draw together findings from a number of research studies to discuss the implications for the development of educational practices in Higher Education towards a student experience which is rich in authenticity.","tags":["Twitter","Data Analysis","Social Media","E-Learning","Education"],"title":"Learning and Knowledge Building with Web 2.0 Technologies: Implications for Teacher Education","type":"publication"}]