[{"authors":["admin"],"categories":null,"content":"I\u0026rsquo;m a Senior Lecturer in Data Journalism at City, University of London.\nPrior to that I was founding co-director of the MSc in Computational and Data Journalism, a joint honours course in journalism and computer science.\nI\u0026rsquo;m also a co-founder of the European Data and Computational Journalism Conference. My interests include data journalism, journalism and software development, social media and production journalism.\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I\u0026rsquo;m a Senior Lecturer in Data Journalism at City, University of London.\nPrior to that I was founding co-director of the MSc in Computational and Data Journalism, a joint honours course in journalism and computer science.","tags":null,"title":"Glyn Mottershead","type":"authors"},{"authors":null,"categories":null,"content":"To get going you\u0026rsquo;ll need to install R, and the RStudio IDE.\nYou can have a play with the cloud version of RStudio, but my students have found some limitations including upload file size issues.\n","date":1586908800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1586908800,"objectID":"ca7d7326c1b6b3f5760eb09e21b78f46","permalink":"/tutorial/tidyverse/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/tutorial/tidyverse/","section":"tutorial","summary":"Learn how to use tidyverse tools for datajournalism.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"A few years back the team at the Financial Times created the Visual Vocabulary tool, download from Github - a typology inspired by the Graphic Continuum.\nI\u0026rsquo;ve been using different tools for a while with my students, but I now start with them with the Visual Vocabulary as it is simple but really useful to help understand how to communicate with the right type of chart or visual.\nAlan Smith, Executive Editor and Head of Visual and Data Journalism, wrote this really interesting post on the FT site explaining why the Visual Vocabulary started, have a look at the image of it - now explainers next to each graph type back at the start.\nI\u0026rsquo;ll build up this section over time, showing some ggplot2 recipes for making some of the different types of chart that fall within the categories:\n Deviation Correlation Ranking Distribution Change over Time Magnitude Part-to-whole Spatial Flow  ","date":1586908800,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1586908800,"objectID":"b2001fb0956dd30c1fb6ae5bce3fbd8e","permalink":"/tutorial/visvocab/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/tutorial/visvocab/","section":"tutorial","summary":"Looking at the FT's Visual Vocabulary in ggplot2","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"What does the Visual Vocabulary say? Deviation -\n \u0026ldquo;Emphasise variations (+/-) from a fixed reference point. Typically the reference point is zero but it can also be a target or a long-term average. Can also be used to show sentiment (positive/neutral/negative). Example FT uses Trade surplus/deficit, climate change\n The diverging bar is:\n \u0026ldquo;A simple standard bar chart that can handle both negative and positive magnitude values.\u0026rdquo;\n Ok, so let\u0026rsquo;s look at how to make one as bar charts are quite simple to make.\nWe\u0026rsquo;ll be using data from the gampinder package - which gives an excerpt from the gapminder dataset. If you don\u0026rsquo;t know about gapminder and the work of Hans Rosling and the rest of the gampinder team - then watch this video - to be fair, I\u0026rsquo;d recommend watching anything that Hans did.\nThe spec from the gapminder pacakge read-me shows the data includes:\n   Variable Meaning     country    continent    year    lifeExp life expectancy at birth   pop\ttotal population   gdpPercap per-capita GDP    We\u0026rsquo;re going to have a look at lifeExp to see if there have been any changes. We\u0026rsquo;ll load up the tidyverse (I could just load up the dplyr, tidyr and ggplot2 packages here) and gapminder.\nlibrary(tidyverse)  ## ── Attaching packages ────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.3.0 ──  ## ✓ ggplot2 3.3.0 ✓ purrr 0.3.3 ## ✓ tibble 3.0.0 ✓ dplyr 0.8.5 ## ✓ tidyr 1.0.2 ✓ stringr 1.4.0 ## ✓ readr 1.3.1 ✓ forcats 0.5.0  ## ── Conflicts ───────────────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag()  library(gapminder)  The library loads the gapminder data as a dataframe but it will not show in your global evironment - let\u0026rsquo;s have a look at it.\nstr(gapminder)  ## tibble [1,704 × 6] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels \u0026quot;Afghanistan\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ continent: Factor w/ 5 levels \u0026quot;Africa\u0026quot;,\u0026quot;Americas\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ year : int [1:1704] 1952 1957 1962 1967 1972 1977 1982 1987 1992 1997 ... ## $ lifeExp : num [1:1704] 28.8 30.3 32 34 36.1 ... ## $ pop : int [1:1704] 8425333 9240934 10267083 11537966 13079460 14880372 12881816 13867957 16317921 22227415 ... ## $ gdpPercap: num [1:1704] 779 821 853 836 740 ...  The latest year we have data for is 2007 - so let\u0026rsquo;s do a comparison across 10 years for countries in Africa and see if life expectancy has gone up or down.\nI\u0026rsquo;m also going to use pivot_wider() to change the shape of the dataframe and I\u0026rsquo;ll use mutate to calculate the difference in years from our new columns.\ngapminder_africa \u0026lt;- gapminder %\u0026gt;% filter(year == 1997 | year == 2007) %\u0026gt;% filter(continent == \u0026quot;Africa\u0026quot;) %\u0026gt;% select(country, year, lifeExp) %\u0026gt;% pivot_wider(id_cols = country, names_from = year, values_from = lifeExp) %\u0026gt;% mutate(life_change = `2007` - `1997`) str(gapminder_africa)  ## tibble [52 × 4] (S3: tbl_df/tbl/data.frame) ## $ country : Factor w/ 142 levels \u0026quot;Afghanistan\u0026quot;,..: 3 4 11 14 17 18 20 22 23 27 ... ## $ 1997 : num [1:52] 69.2 41 54.8 52.6 50.3 ... ## $ 2007 : num [1:52] 72.3 42.7 56.7 50.7 52.3 ... ## $ life_change: num [1:52] 3.15 1.77 1.95 -1.83 1.97 ...  The first chart We could create a new column which says the data is above or below zero - but we can use the fill = life_change \u0026gt; 0 within aes() to do that for us. Have a look at the code below.\nggplot(data = gapminder_africa, aes(x = country, y = life_change, fill = life_change \u0026gt; 0))+ geom_bar(stat = \u0026quot;identity\u0026quot;)  It has done what we wanted, but the order is set by the x axis, not what we want. Also, the x axis looks a mess - lots of names that are running into one another.\nSo, we\u0026rsquo;ll take the data set we want to work with and set the aes() like this:\n reorder the x axis by the life_change column - set it to the second argument in the reorder function set the y axis using the life_change values the fill line (life_change \u0026gt; 0) will set the data into two groups - above and below 0  We\u0026rsquo;ll also add coord_flip() to change it from a horizontal to a vertical chart.\nggplot(data = gapminder_africa, aes(x = reorder(country, life_change), y = life_change, fill = life_change \u0026gt; 0))+ geom_bar(stat = \u0026quot;identity\u0026quot;) + coord_flip()  Mutating the data In the second version I\u0026rsquo;m going to create a column that contains the information about the changes using mutate(). This means the aes() can be filled by the life_change_pos column we\u0026rsquo;ll create. This is just to show how it is done.\nI\u0026rsquo;ve also added a few more stylistic points to make it look nicer. Have a look at the section after coord_flip() to see what is going on.\ngapminder_africa2 \u0026lt;- gapminder_africa %\u0026gt;% mutate(life_change_pos = life_change \u0026gt; 0) ggplot(data = gapminder_africa2, aes(x = reorder(country, life_change), y = life_change, fill = life_change_pos))+ geom_bar(stat = \u0026quot;identity\u0026quot;) + coord_flip() + labs(x = \u0026quot;Country\u0026quot;, y = \u0026quot;Change in Life Expectancy (years)\u0026quot;, title = \u0026quot;Life Expectancy change in Africa\u0026quot;, subtitles = \u0026quot;Comparing 2007 to 1997 using Gapminder data\u0026quot;)+ theme_minimal()+ guides(fill = FALSE) + theme(panel.border = element_blank(), panel.grid.major = element_blank(), panel.grid.minor = element_blank())  ","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587945600,"objectID":"7bd34b24c84325c628ed7bef408f263e","permalink":"/tutorial/visvocab/diverging_bar/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/tutorial/visvocab/diverging_bar/","section":"tutorial","summary":"What does the Visual Vocabulary say? Deviation -\n \u0026ldquo;Emphasise variations (+/-) from a fixed reference point. Typically the reference point is zero but it can also be a target or a long-term average.","tags":null,"title":"Pivot tables","type":"docs"},{"authors":null,"categories":null,"content":"Looking at police data using Tidyverse tools The Tidyverse tools are one of my favourite things and made my R coding experience much simpler. Most of the things I\u0026rsquo;ll be posting will use them in one way or another.\nWe\u0026rsquo;re going to find out what happens when someone reports a stolen bicycle - a friend seems to believe the police do nothing, so let\u0026rsquo;s investigate what actually happens when a crime is reported.\nDownloading the data I\u0026rsquo;ve downloaded a month\u0026rsquo;s worth of data for South Wales Police from data.police.uk.\nI\u0026rsquo;m going to do the R equivalent of a pivot table, grouping information together and then analysing it.\nGetting started Open RStudio and go to FILE \u0026gt; NEW PROJECT then create it in a new folder. This will give you a chance to choose where you want it to go. I tend to have a project folder and then each individual project is stored inside that.\nWe\u0026rsquo;ll leave RStudio for a second and head into the computer\u0026rsquo;s file manager to find the folder we just created in RStudio. Inside that I always create a folder called data, I\u0026rsquo;ll then put my downloaded data inside it.\nCreate a script Back inside RStudio, I\u0026rsquo;m going to go to File menu \u0026gt; New File \u0026gt; R Script. I\u0026rsquo;m doing this as it allows me to repeat this analysis when I want to. You\u0026rsquo;ll want to give it a name that makes sense and then save it. R files have a .R file extension at the end.\nInstalling your first R Package A package is a tool, or group of tools, that someone has created to cut out some of the repetition in R. We\u0026rsquo;ll be using the tidyverse by Hadley Wickham. These are a set of really helpful data tools that also make R programming easier to read.\nWe can install packages by going to the Tools menu \u0026gt; Install Packages and type the name into the pop-up box.\nOr I can write the command (known as a function) into my script, highlight my line of code and hit the RUN button at the top of the script. The other way to do it is a line of code - install.packages(\u0026quot;tidyverse\u0026quot;).\nI then need to load it in my script. This is done by the library() function. Again, run the line of code.\nlibrary(tidyverse)  Loading data Even though we have the data set in our R project folder we need to load it up.\nI\u0026rsquo;m going to use read_csv() from readr to read a CSV file into our RStudio environment.\nIf you ever want to know how what a package or function does then type ?function_name in the console eg ?read_csv, and you\u0026rsquo;ll get a help message.\nOur function wrapping works like this read_csv().\nWe need to put the address (** aka file path**) for our CSV inside quote marks inside the inner pair of brackets. It will look like this - read_csv(\u0026quot;file_path\u0026quot;).\nThe full syntax (not the working code I want to use) looks like this: data_frame_name \u0026lt;- read_csv(\u0026quot;file_path\u0026quot;)\nThe \u0026lt;- is an assignment symbol, I tend to think of it as a hosepipe which allows the function to pour into the name we give our data frame (and yes, the analogy continues as I think of the frame name as a bucket for storing things in - so the \u0026lt;- allows us to pour data into where it is being stored).\nMy file path in the example below says ./ inside the folder we are currently in is a folder called data and / inside that is a named csv file.\ncrimes \u0026lt;- read_csv(\u0026quot;./data/2019-01-south-wales-street.csv\u0026quot;)  ## Parsed with column specification: ## cols( ## `Crime ID` = col_character(), ## Month = col_character(), ## `Reported by` = col_character(), ## `Falls within` = col_character(), ## Longitude = col_double(), ## Latitude = col_double(), ## Location = col_character(), ## `LSOA code` = col_character(), ## `LSOA name` = col_character(), ## `Crime type` = col_character(), ## `Last outcome category` = col_character(), ## Context = col_logical() ## )  The output for this will show us what types of columns we have - in this case col_character() means text and col_double() means a number format. All good so far.\nFiltering with dplyr I want to filter the information so it just includes bicycle thefts. I\u0026rsquo;m going to use some dplyr package tricks to do this.\nI\u0026rsquo;m going to store it in a new data frame called bikes. We\u0026rsquo;ll be using the crimes data frame we made earlier and then filtering it, before assigning the new information to bikes. So bikes \u0026lt;- crimes.\nThe next bit of the code is %\u0026gt;% a pipe - essentially you can say and then to yourself when you write it. Our next function is a filter(), it does a similar job to the Excel function of the same name.\nMost functions will need you to say which data frame you are working on and then what you want to do. So we could say filter(data_frame, column_name == \u0026quot;text we want back\u0026quot;).\nBut dplyr knows we are working on the data frame before the first %\u0026gt;%, so our code becomes simpler and we just need the column name and the text to grab.1\nbikes \u0026lt;- crimes %\u0026gt;% filter(`Crime type` == \u0026quot;Bicycle theft\u0026quot;) # If I use the dataframe name on its own and run that line I can see what is in it bikes  ## # A tibble: 126 x 12 ## `Crime ID` Month `Reported by` `Falls within` Longitude Latitude Location ## \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; ## 1 0fad86217… 2019… South Wales … South Wales P… -3.54 51.5 On or n… ## 2 5ea5f1bb3… 2019… South Wales … South Wales P… -3.19 51.5 On or n… ## 3 ab5c7a5c7… 2019… South Wales … South Wales P… -3.12 51.5 On or n… ## 4 44e702929… 2019… South Wales … South Wales P… -3.23 51.5 On or n… ## 5 979ee8076… 2019… South Wales … South Wales P… -3.09 51.5 On or n… ## 6 34490753e… 2019… South Wales … South Wales P… -3.12 51.5 On or n… ## 7 59c9863ef… 2019… South Wales … South Wales P… -3.12 51.5 On or n… ## 8 7029c550b… 2019… South Wales … South Wales P… -3.22 51.5 On or n… ## 9 f8f063bad… 2019… South Wales … South Wales P… -3.22 51.5 On or n… ## 10 cdd878455… 2019… South Wales … South Wales P… -3.20 51.5 On or n… ## # … with 116 more rows, and 5 more variables: `LSOA code` \u0026lt;chr\u0026gt;, `LSOA ## # name` \u0026lt;chr\u0026gt;, `Crime type` \u0026lt;chr\u0026gt;, `Last outcome category` \u0026lt;chr\u0026gt;, ## # Context \u0026lt;lgl\u0026gt;  The output will show us how many rows (observations) and columns (variables) there are - in this case a 126 x 12. We can start adding some more analysis now.\nFiltering and renaming for speed You may notice when we printed the data frame that some of the column names have backticks around them. That\u0026rsquo;s because they\u0026rsquo;ve got a space between the words and R is trying to help out. So, if we wanted to work with our Crime ID column we\u0026rsquo;d actually have to type Crime ID with the back ticks.\nI\u0026rsquo;m too lazy for things like that. I\u0026rsquo;d rather change the name so I can write something easier. We\u0026rsquo;re going to use rename() to do it. We will tell rename() that our \u0026quot;new column name\u0026quot; = \u0026quot;old column name\u0026quot; - but make sure you put the quote marks around the names when you do this.\nSo, I\u0026rsquo;m going to tweak my code from before. I\u0026rsquo;m going to change the name of the Last outcome category and the Crime type columns. I\u0026rsquo;m going to change them to Outcome and Crimes - much shorter to type.\nSo, we\u0026rsquo;re telling dplyr to take the crimes data frame and then rename by assigning the new name to the old name. (The comma means we can do more than one rename at once, to do more just add a comma and then say \u0026ldquo;new\u0026rdquo; = \u0026ldquo;old\u0026rdquo; again.) And then filter our newly-named Crimes column and look for the text inside the quote marks.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;)  Doing a pivot table in dplyr syntax Now we can do some digging. I want to know what the police have done about the reported bike thefts.\nWe can group things together by our Outcome category and count how often each of the outcomes happen. So, we tell group_by() which column we want to work on. And then we tell it what to count - in this case it creates a column called outcome_count and uses the n() function to count the Outcome.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;) %\u0026gt;% group_by(Outcome) %\u0026gt;% summarise(outcome_count = n()) # Again I'll type the dataframe name here and run this line to see what we have got. bikes  ## # A tibble: 4 x 2 ## Outcome outcome_count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 Awaiting court outcome 1 ## 2 Investigation complete; no suspect identified 88 ## 3 Unable to prosecute suspect 1 ## 4 Under investigation 36  We\u0026rsquo;ve got a pivot table - it has grouped the Outcome column and counted how often each one occurs.\nTLDR: horrible description names are horrible We\u0026rsquo;re going to change an element by using the mutate() function - we\u0026rsquo;re going to recode() one of our Outcome categories to shorten it. The code would look like this - mutate(Outcome = recode(Outcome, \u0026quot;Investigation complete; no suspect identified\u0026quot; = \u0026quot;No suspect identified\u0026quot;))\nBut I\u0026rsquo;d like to order it in descending order - so we can do one more and then %\u0026gt;% at the end of our code. We will arrange() our outcome_count in desc() - descending order. You must put desc() inside the arrange() function. And check your brackets.\nbikes \u0026lt;- crimes %\u0026gt;% rename(\u0026quot;Outcome\u0026quot; = \u0026quot;Last outcome category\u0026quot;, \u0026quot;Crimes\u0026quot; = \u0026quot;Crime type\u0026quot;) %\u0026gt;% filter(Crimes == \u0026quot;Bicycle theft\u0026quot;) %\u0026gt;% mutate(Outcome = recode(Outcome, \u0026quot;Investigation complete; no suspect identified\u0026quot; = \u0026quot;No suspect identified\u0026quot;)) %\u0026gt;% group_by(Outcome) %\u0026gt;% summarise(outcome_count = n()) %\u0026gt;% arrange(desc(outcome_count)) # Again I'll type the dataframe name here and run this line to see what we have got. bikes  ## # A tibble: 4 x 2 ## Outcome outcome_count ## \u0026lt;chr\u0026gt; \u0026lt;int\u0026gt; ## 1 No suspect identified 88 ## 2 Under investigation 36 ## 3 Awaiting court outcome 1 ## 4 Unable to prosecute suspect 1  And we\u0026rsquo;ve got it. A pivot table that tells us what has happened to the bike crimes in the original data set and has put them in descending order.\n  The text we want back has to be surrounded in quote marks \u0026quot;\u0026quot; as we are looking at text, or a string as it is more properly known. Find out about string manipulation from the R for Data Science book. \u0026#x21a9;\u0026#xfe0e;\n   ","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"db40ec0cf3510f75737c8200e53e3f20","permalink":"/tutorial/tidyverse/pivot-tables-in-r/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/tutorial/tidyverse/pivot-tables-in-r/","section":"tutorial","summary":"Looking at police data using Tidyverse tools The Tidyverse tools are one of my favourite things and made my R coding experience much simpler. Most of the things I\u0026rsquo;ll be posting will use them in one way or another.","tags":null,"title":"Pivot tables","type":"docs"},{"authors":null,"categories":null,"content":"Getting the data We can, of course, download the data set we want and then use tools such as readr (if it is a csv file) to import the data. But that misses out on quite a powerful way of grabbing information. Many organisations have set up a way of asking questions directly to their database - think about it like a machine-to-machine conversation.\nThis is called an API (Application Programming Interface), and there are lots of them about. So, a basic understanding of how to get that information into an R dataframe for analysis is really important.\nAnd just for an added bonus, we\u0026rsquo;re going to build on the concepts from the pivot table session to add in a graph using ggplot2 (again a tidyverse tool).\nTools for the job We\u0026rsquo;ll be using httr to work with our APIs - it makes it much easier to construct a question. I\u0026rsquo;ve tried using the paste() function to do this in the past and didn\u0026rsquo;t find it easy, httr is much nicer in comparison.\nThe response from our questions is often in the form of either JSON or XML. The documentation for the API will tell you about what form it is in and that will tell us what tools we need to deal with the particular formats.\nWe\u0026rsquo;ll be using an API from the data.police site, which returns data from queries as JSON - we\u0026rsquo;ll need the jsonlite package to help us with that.\nThe final package is the tidyverse to allow us to do some data changes, in a similar way to what we\u0026rsquo;ve done before and to generate the chart we are going to make at the end.\nSo, usual rules create an R script file in RStudio and we can get working.\nlibrary(httr) library(jsonlite) library(tidyverse)  What are we looking for? The first step is to look at what kind of crimes we can investigate. The documentation for the data.police site\u0026rsquo;s API gives this examples of the different API queries that are available. You can just copy and paste one of the examples into a web browser to see what is going on. I\u0026rsquo;ve picked the crime categories API page - which shows an example that looks like this - https://data.police.uk/api/crime-categories?date=2020-03. Copy and paste that into a web browser to get a JSON view of the data.\nI\u0026rsquo;ve had a look at the webpage and inspired by what I\u0026rsquo;ve seen in the news recently, I\u0026rsquo;m going to look at car crime around the main hospital in Cardiff. You could swap any category in this.\nBuilding our query web address We\u0026rsquo;ll be looking at street level crimes, this call is built is in three parts:\n The basic structure https://data.police.uk/api/crimes-street/ The crime type vehicle-crime The query lat= long= date=  I\u0026rsquo;ll go onto Google Maps in Chrome and search for University Hospital of Wales - the hospital mentioned in the story I read. If you right click on the point you searched for in the browser window, you can pick What's here? and you\u0026rsquo;ll get a little pop-up with the latitude and longitude on the bottom. Copy that over to your script.\nWe\u0026rsquo;re going to store the first part of the address in an object called url. I\u0026rsquo;ve copied an example from the documentation but changed it from the usual all-crimes? to vehicle-crime?.\nWe\u0026rsquo;ll wrap the web address in double quotes and use the \u0026lt;- assign symbol.\nurl \u0026lt;- \u0026quot;https://data.police.uk/api/crimes-street/vehicle-crime?\u0026quot;  We\u0026rsquo;ll now start structuring our request to the data.police server. We\u0026rsquo;ll be using GET() from httr - it does what it says. We\u0026rsquo;ll tell it what the url is that we are using and then give it a list() with our query elements inside it. Have a play with the dates and see what is going on - you may get error messages. If so, Google them and find out what they mean.\nAn important point in checking your code is watching out for how many brackets you have. This is a real source of broken code for beginners, so watch out for it. This section below has two opening brackets - one for GET and one for list, so we must have matching closing brackets. If it isn\u0026rsquo;t matched the code will not run.\nIn mock structure it looks like this - our_stored_call \u0026lt;- GET(base_address, query = list(some queries, separated by commas)). I\u0026rsquo;ve hit return at points below to make it easier to read, it will be treated as one line of code like this.\napi_call \u0026lt;- GET(url = url, query = list( lat = 51.50724, lng = -3.190278, date = \u0026quot;2020-02\u0026quot;) )  The key thing to do when you make a call to an API is to check what has happened - they all have a status code built into the return. We can access this by running one line of code - api_call$status_code. It will return a number - 200 means all is well, numbers in the 400s usually mean you haven\u0026rsquo;t structured your call properly and 500s means their API isn\u0026rsquo;t working.\nWe\u0026rsquo;ve got a 200 - all is well.\napi_call$status_code  ## [1] 200  The way JSON is structured means there is a header with lots of background information (metadata) and content - which is what we want. We\u0026rsquo;ll use jsonlite\u0026rsquo;s content() function and specify how we want it back.\nresponse \u0026lt;- content(api_call, as = \u0026quot;text\u0026quot;, encoding = \u0026quot;UTF-8\u0026quot;)  I wouldn\u0026rsquo;t normally use head() to look at the top of this kind of object but, let\u0026rsquo;s do it here to see what we get back. You can call head() on many types of data object in R to see what you\u0026rsquo;ve got at the head (top) of it. You can even tell it how many to look at by adding an n = * argument eg head(object, n = 6).\nhead(response)  ## [1] \u0026quot;[{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.510986\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1082838,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Heol Wernlas\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.207382\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Under investigation\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;cb7458646b734199c1728e7ba2b0cc375cb792a1d181092c30e3a8a217838c32\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238108,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.504594\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1083004,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Dovedale Close\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.169997\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-03\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;1b5a72e887633314e5f0f570e97a8112bc99781745db1b3c54f3a0815117628f\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238172,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.497118\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081632,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Petrol Station\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.192711\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-03\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;b66c1dcefad0ced8cffde6924ae86bf36df004c84c48389c15db6e44a99baff6\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238305,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.515486\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1083744,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Heath Park Lane\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.182295\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;4d918317cd772b800243401b2deba7a91f11a86f9da0080d6dd9d84d4b9ef83f\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81239441,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.497118\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081632,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Petrol Station\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.192711\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Under investigation\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;2b2163a342579acc4b7d457b1a270a26acb7cc116ff948c8e35628bddd4e005a\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238244,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.497118\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081632,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Petrol Station\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.192711\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;3df4e80f4bcd590c1760c5df0be5755ec90d7acb5711e23ecc1b07c4d263ba6b\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81239576,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.496468\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081601,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Rhygoes Street\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.184972\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;e13bff04cc19a9f8331d4e1cff06ec4d132ca636e728f465f802aa8a187835ae\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81239620,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.496485\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081626,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Brithdir Street\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.184180\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;f9443b6ebbb08eb279883f4cfd79947c988edccda3b2615dcc5d30e698eeb496\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238098,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;vehicle-crime\\\u0026quot;,\\\u0026quot;location_type\\\u0026quot;:\\\u0026quot;Force\\\u0026quot;,\\\u0026quot;location\\\u0026quot;:{\\\u0026quot;latitude\\\u0026quot;:\\\u0026quot;51.495872\\\u0026quot;,\\\u0026quot;street\\\u0026quot;:{\\\u0026quot;id\\\u0026quot;:1081618,\\\u0026quot;name\\\u0026quot;:\\\u0026quot;On or near Gelligaer Gardens\\\u0026quot;},\\\u0026quot;longitude\\\u0026quot;:\\\u0026quot;-3.187881\\\u0026quot;},\\\u0026quot;context\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;outcome_status\\\u0026quot;:{\\\u0026quot;category\\\u0026quot;:\\\u0026quot;Investigation complete; no suspect identified\\\u0026quot;,\\\u0026quot;date\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;},\\\u0026quot;persistent_id\\\u0026quot;:\\\u0026quot;a0e5ca870b8a6c756b28529e7e6554ae499810a12ccf0e3e33be946fd48eb18c\\\u0026quot;,\\\u0026quot;id\\\u0026quot;:81238168,\\\u0026quot;location_subtype\\\u0026quot;:\\\u0026quot;\\\u0026quot;,\\\u0026quot;month\\\u0026quot;:\\\u0026quot;2020-02\\\u0026quot;}]\u0026quot;  That\u0026rsquo;s great, notice how each individual reported crime is wrapped in {}? We can get that out of JSON and into a dataframe like this: We\u0026rsquo;ll use fromJSON() and tell it what object to work on, and we want to flatten it. We can use a pipe %\u0026gt;% to add another command and this one will be to set it up as a data.frame().\nvehicle_crimes \u0026lt;- fromJSON(response, flatten = TRUE) %\u0026gt;% data.frame() # let's have a look at the structure of the dataframe str(vehicle_crimes)  ## 'data.frame':\t9 obs. of 13 variables: ## $ category : chr \u0026quot;vehicle-crime\u0026quot; \u0026quot;vehicle-crime\u0026quot; \u0026quot;vehicle-crime\u0026quot; \u0026quot;vehicle-crime\u0026quot; ... ## $ location_type : chr \u0026quot;Force\u0026quot; \u0026quot;Force\u0026quot; \u0026quot;Force\u0026quot; \u0026quot;Force\u0026quot; ... ## $ context : chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ... ## $ persistent_id : chr \u0026quot;cb7458646b734199c1728e7ba2b0cc375cb792a1d181092c30e3a8a217838c32\u0026quot; \u0026quot;1b5a72e887633314e5f0f570e97a8112bc99781745db1b3c54f3a0815117628f\u0026quot; \u0026quot;b66c1dcefad0ced8cffde6924ae86bf36df004c84c48389c15db6e44a99baff6\u0026quot; \u0026quot;4d918317cd772b800243401b2deba7a91f11a86f9da0080d6dd9d84d4b9ef83f\u0026quot; ... ## $ id : int 81238108 81238172 81238305 81239441 81238244 81239576 81239620 81238098 81238168 ## $ location_subtype : chr \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; \u0026quot;\u0026quot; ... ## $ month : chr \u0026quot;2020-02\u0026quot; \u0026quot;2020-02\u0026quot; \u0026quot;2020-02\u0026quot; \u0026quot;2020-02\u0026quot; ... ## $ location.latitude : chr \u0026quot;51.510986\u0026quot; \u0026quot;51.504594\u0026quot; \u0026quot;51.497118\u0026quot; \u0026quot;51.515486\u0026quot; ... ## $ location.longitude : chr \u0026quot;-3.207382\u0026quot; \u0026quot;-3.169997\u0026quot; \u0026quot;-3.192711\u0026quot; \u0026quot;-3.182295\u0026quot; ... ## $ location.street.id : int 1082838 1083004 1081632 1083744 1081632 1081632 1081601 1081626 1081618 ## $ location.street.name : chr \u0026quot;On or near Heol Wernlas\u0026quot; \u0026quot;On or near Dovedale Close\u0026quot; \u0026quot;On or near Petrol Station\u0026quot; \u0026quot;On or near Heath Park Lane\u0026quot; ... ## $ outcome_status.category: chr \u0026quot;Under investigation\u0026quot; \u0026quot;Investigation complete; no suspect identified\u0026quot; \u0026quot;Investigation complete; no suspect identified\u0026quot; \u0026quot;Investigation complete; no suspect identified\u0026quot; ... ## $ outcome_status.date : chr \u0026quot;2020-02\u0026quot; \u0026quot;2020-03\u0026quot; \u0026quot;2020-03\u0026quot; \u0026quot;2020-02\u0026quot; ...  We can now start looking at what we\u0026rsquo;ve got going on in our dataset. First off, we\u0026rsquo;re going to get rid of some of the columns we don\u0026rsquo;t need. We\u0026rsquo;ll be overwriting the data by storing it in the same dataframe. dplyr\u0026rsquo;s select() allows us to pick what we want and give it a new name into the bargain - we used rename() last time. Both work.\nI tend to keep IDs for matching with things later, to check crime outcomes - but today for one time only we\u0026rsquo;ll get rid of it.\nvehicle_crimes \u0026lt;- select(vehicle_crimes, month, category, location = location.street.name, long = location.longitude, lat = location.latitude, outcome = outcome_status.category)  Great, so we\u0026rsquo;ll group the crimes to see what is happening - we could do this on any one of the columns but for now we\u0026rsquo;ll do it on outcome.\nWe\u0026rsquo;ll use mutate() and recode() to simplify one of the descriptions (old = new), then we\u0026rsquo;ll z to get the unique objects and summarise() by counting (n()) the number of times it occurs before we arrange() in descending (desc()) order.\noutcomes \u0026lt;- vehicle_crimes %\u0026gt;% mutate(outcome = recode(outcome, \u0026quot;Investigation complete; no suspect identified\u0026quot; = \u0026quot;No suspect identified\u0026quot;)) %\u0026gt;% group_by(outcome) %\u0026gt;% summarise(outcome_count = n()) %\u0026gt;% arrange(desc(outcome_count))  A really simple chart One of the tools I use a lot, and get all of my students to use, is the FT\u0026rsquo;s Visual Vocabulary which was designed to help the FT\u0026rsquo;s team pick the right kind of tool for the job. I\u0026rsquo;ll be posting more about data visualisation soon.\nI\u0026rsquo;m going to build a really simple column chart, using ggplot2. ggplot() has a + symbol which is like the pipe from dplyr and means AND THEN. So, we tell ggplot() what dataframe to work on, and what data to work on for the axes (aes()) and then what type of chart to work with.\nggplot(outcomes, aes(x = outcome, y = outcome_count)) + geom_col()  ggplot2 has loads of options to play with to make a better looking chart, but this will give you an idea and an introduction to it. We\u0026rsquo;ll look at this more in due course.\nConclusion As well as importing files, it is possible to get data straight from a web server that allows it using an API - here we\u0026rsquo;ve built an API call, got JSON data back and turned it into a dataframe before going over the pivot table (group by function) and introducing a simple chart.\n","date":1586908800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586908800,"objectID":"0d4614e1e531533f1f23c84df5d98127","permalink":"/tutorial/tidyverse/first-graph/","publishdate":"2020-04-15T00:00:00Z","relpermalink":"/tutorial/tidyverse/first-graph/","section":"tutorial","summary":"Getting the data We can, of course, download the data set we want and then use tools such as readr (if it is a csv file) to import the data. But that misses out on quite a powerful way of grabbing information.","tags":null,"title":"First graph in ggplot","type":"docs"},{"authors":[],"categories":["coding","beginners","Data Journalism"],"content":"Investigative data journalist Leila Haddou joined us in #datalockdown to deliver a lecture to data and investigative journalism students from City, University of London, Birmingham City University, and De Montfort University on April 30.\nOne point that came up during the session is that a lot of coding resources are not aimed at journalists. So, I thought I\u0026rsquo;d start pulling together a list of sources and some thoughts about them.\nThe initial version of this post will be biased towards R, as that is what I tend to work in. I\u0026rsquo;ll add Python and other resources as I go along.\nAlso, it doesn\u0026rsquo;t matter if you call it coding, programming or any of the other terms - I\u0026rsquo;m going to assume that we are talking about the data journalism workflow (acquiring, cleaning, analysing and displaying and equivalents) rather than building apps or websites.\nWant to add anything? Tweet @glynmottershead with the hashtag #codereadinglist.\nCross language Datacamp has a lot of materials to support training, you can pay for an account or if you are an educator it is possible for you to register your class for educational access. It\u0026rsquo;s really useful but one thing to point out is it is aimed at data scientists rather than journalists. This can mean that some of the exercises may not seem obvious at first, but stick with it.\nWebsites covering a range of related topics   Paul Bradshaw\u0026rsquo;s Online Journalism Blog has lost of great pointers, and a list of his books. I\u0026rsquo;ll break those down over time.  Courses Any course listed is not necessarily a recommendation.\n Intro to R for Journalists at journalismcourses.org by Andrew Ba Tran of the Wall Street Journal. An excellent suggestion by Paul Bradshaw.\nGroups  Journocoders is a London group I would recommend attending when lockdown is finished. Leila Haddou is one member of the team behind it. Think friendly group of people, coding, beers/soft drinks and a good supportive atmosphere. I go when I can and really enjoy it.\nR Programming Books   Sharon Machlis has put six chapters of her book R for Mass Communication and Jouranlism online for free. This is a brilliant book on R for Data Science by Hadley Wickham and Garrett Grolemund. It covers a lot of the key principles required by data journalists, but again the examples might not always resonate. I always recommend getting to grips with the tidyverse suite of tools (covered in the book), as it makes your code a lot easier to read.  Websites  I\u0026rsquo;m going to add my site here as the tutorial section is growing over time, and is focused on coding in R for journalists  Python programming   Automate the boring stuff is a Python book aimed at \u0026ldquo;practical programming for absolute beginners\u0026rdquo; - a recommendation from Mindy McAdams. There\u0026rsquo;s also a course on Udemy, the price seems to fluctuate quite a bit so keep an eye out.  First Python Notebook - a step-by-step guide to analyzing data with Python and the Jupyter notebook. Suggested by Paul Bradshaw.  Data visualisation  Nathan Yau\u0026rsquo;s site [Flowingdata] (https://flowingdata.com/) has tutorials and a subscription/membership option. I\u0026rsquo;ve done this in the past and found it really helpful.\nOther tools and languages Although I point out this list isn\u0026rsquo;t about web dev - if you want to understand web scraping then an understanding of web page structure is important, so I recommend w3 schools as one starting point.\n","date":1588204800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588327274,"objectID":"6ebd6e8889eac4b0be4f89061d9f8325","permalink":"/post/coding-for-journalists/","publishdate":"2020-04-30T00:00:00Z","relpermalink":"/post/coding-for-journalists/","section":"post","summary":"Investigative data journalist Leila Haddou joined us in #datalockdown to deliver a lecture to data and investigative journalism students from City, University of London, Birmingham City University, and De Montfort University on April 30.","tags":["python","rstats"],"title":"Coding for journalists - an evolving reading list","type":"post"},{"authors":[],"categories":["Data Journalism","Datajconf"],"content":"You\u0026rsquo;ve probably seen the news by now that we\u0026rsquo;ve had to postpone the European Data and Computational Journalism Conference due to Covid-19.\nI would have posted earlier, but an upgrade and site rebuild means I haven\u0026rsquo;t done much posting here for a while.\nPlans to host the conference at City, University of London (my current place of employment) were in full swing when the organising committee had to take the decision to cancel due to everyone\u0026rsquo;s safety.\nOur plan is a break rather than a farewell, but if you want to have a chat with us between now and DataJConf 2021, then please get in touch!\n","date":1588032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1588091253,"objectID":"c80ef1eed309f3a939f7d384e0f77ca0","permalink":"/post/covid-19-and-the-european-data-journalism-conference/","publishdate":"2020-04-28T00:00:00Z","relpermalink":"/post/covid-19-and-the-european-data-journalism-conference/","section":"post","summary":"You\u0026rsquo;ve probably seen the news by now that we\u0026rsquo;ve had to postpone the European Data and Computational Journalism Conference due to Covid-19.\nI would have posted earlier, but an upgrade and site rebuild means I haven\u0026rsquo;t done much posting here for a while.","tags":["data journalism","Datajconf"],"title":"Covid-19 and the European Data and Computational Journalism Conference","type":"post"},{"authors":[],"categories":["open data","Data Journalism"],"content":"I\u0026rsquo;ve just working on a couple of tutorials using data from data.police.uk - the first using the csv download method and the second using an API call using the httr package.\nHowever, you call the data you get back a whole host of information - including the latitude and longitude of the incidents themselves.\nOr do you?\nLast year, one of my former students Conor Gogarty wrote this story on the GloucestershireLive site following up from a crime map that showed - \u0026ldquo;My search for crime in the Gloucester crime hotspot with no crime\u0026rdquo;.\nConor, with his pair of binoculars, goes to investigate what is going on in Tredworth\u0026rsquo;s Bishop\u0026rsquo;s Castle Way.\nHe found really interesting information including these gems:\n \u0026ldquo;A total of 41 shoplifting reports were linked to the spot last year. But there are no shops there\u0026hellip; How could 41 shoplifting incidents have been reported last year if the nearest stores were a five-minute walk away in Barton Street?\u0026rdquo;\n As any journalist should, Conor gets in touch with the police - a press officer confirms there were no crimes in that area.\n \u0026ldquo;She said the markers on the map are not the exact locations of offences. They are ‘snap points’ which pull in the reports from nearby spots.\u0026rdquo;\n A further check led to this information:\n \u0026ldquo;Gloucestershire police said this was a question for the Home Office, which oversees the police.uk website. A Home Office spokesman told me inaccuracies can result from “inconsistent geocoding policies”. He said the website makes it clear the data is not always “fully accurate”.\u0026rdquo;\n RTFM When you read the docs, this is an issue that is highlighted:\n \u0026ldquo;The latitude and longitude locations of Crime and ASB incidents published on this site always represent the approximate location of a crime — not the exact place that it happened.\u0026rdquo;\n That\u0026rsquo;s pretty straightforward, when you think about some offences the location could actually help identify the victim of the incident. So, obfuscation is a really good idea.\nThe documentation is really interesting when they explain how the location is anonymised.\nThis is not the location you are looking for  How are crime locations anonymised? We maintain a master list of anonymous map points. Each map point is specifically chosen so that it:\n Appears over the centre point of a street, above a public place such as a Park or Airport, or above a commercial premise like a Shopping Centre or Nightclub. Has a catchment area which contains at least eight postal addresses or no postal addresses at all. When crime data is uploaded by police forces, the exact location of each crime is compared against this master list to find the nearest map point. The co-ordinates of the actual crime are then replaced with the co-ordinates of the map point. If the nearest map point is more than 20km away, the co-ordinates are zeroed out. No other filtering or rules are applied.    How was the master list of snap points created? The snap points list was created in 2012 and based on Ordnance Survey population and housing developments relevant to that year. In summary, to create the master list of anonymous points, we:\n Took the centre point of every road in England and Wales from the Ordnance Survey Locator dataset. Augmented these with locally relevant points of interest from the Point X dataset. Subsequently analysed each map point to see how many postal addresses were contained in its catchment area according to the Ordnance Survey Address-Point dataset. Any with between 1 and 7 postal addresses were discarded to protect privacy. The remaining points were provided to police forces for a human assessment. A small number of additions and deletions were made based on their feedback to make the map points more locally relevant.   Ok, so as data journalists it does mean we have to subject the data to scrutiny before publishing crime near x stories. It also calls into question apps that draw on the data for things like home sales.\nSo, a simple point. Always read the manual for the data - and if you are interested in the data.police.uk site then this academic paper is worth a read if you are able to get access.\n","date":1587945600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1587969598,"objectID":"289ae968e2e5c777b0ce9ba63e96b337","permalink":"/post/data-police-location-issues/","publishdate":"2020-04-27T00:00:00Z","relpermalink":"/post/data-police-location-issues/","section":"post","summary":"I\u0026rsquo;ve just working on a couple of tutorials using data from data.police.uk - the first using the csv download method and the second using an API call using the httr package.","tags":["api","data.police","data journalism"],"title":"Data Police location issues","type":"post"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1562677200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1562677200,"objectID":"231bf7df6a5f4cbae39a895b37e5f5d7","permalink":"/talk/wjec-paris/","publishdate":"2019-03-07T00:00:00Z","relpermalink":"/talk/wjec-paris/","section":"talk","summary":"There's a skills gap between traditional journalism education and training and the emerging requirements of the industry.","tags":["Journalism Education"],"title":"Bridging the Skills Gap: Innovation in Journalism Education","type":"talk"},{"authors":[],"categories":["Datajconf","Data Journalism"],"content":"A video of the conference \n","date":1562284800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1586955680,"objectID":"2a35fa17ce93ea0dcf15ecdc29f610dc","permalink":"/post/2019-07-05-data-journalism-in-malaga/data-journalism-in-malaga/","publishdate":"2019-07-05T00:00:00Z","relpermalink":"/post/2019-07-05-data-journalism-in-malaga/data-journalism-in-malaga/","section":"post","summary":"A video of the conference","tags":["data journalism","Datajconf","ddj"],"title":"Data Journalism in Malaga","type":"post"},{"authors":["Glyn Mottershead"],"categories":null,"content":"","date":1554381000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554381000,"objectID":"74e6a49cd95b6782e0b5665eb8dd9b8d","permalink":"/talk/digital-city/","publishdate":"2019-03-07T00:00:00Z","relpermalink":"/talk/digital-city/","section":"talk","summary":"Data, algorithms, chatbots and computational journalism impacting on newsrooms, are you ready?","tags":[],"title":"Data and Computational Journalism","type":"talk"},{"authors":["Glyn Mottershead","Martin Chorley"],"categories":null,"content":"","date":1543622400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543622400,"objectID":"2d93fe7431ea20f085cd80d300c6ac11","permalink":"/publication/nieman-predictions-2019/","publishdate":"2018-12-01T00:00:00Z","relpermalink":"/publication/nieman-predictions-2019/","section":"publication","summary":"Every time a free tool goes freemium or a platform closes because it can’t make money, we’ve got an issue. What’s going to happen to the stories that tool feeds?","tags":["Journalism","Data Journalism","News Writing","Feature Writing","Journalism Education"],"title":"When a tech company pulls the plug on your story","type":"publication"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1535756400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535756400,"objectID":"79f483fecb548075e055a280902e02cf","permalink":"/publication/are-you-talking-book-chapter/","publishdate":"2018-09-01T00:00:00+01:00","relpermalink":"/publication/are-you-talking-book-chapter/","section":"publication","summary":"This preliminary research aims to show how computational analysis allows us to investigate the means by which news stories are spread.","tags":["Twitter","Data Analysis","Social Media","Journalism Studies"],"title":"Are You Talking To Me?: An Analysis of Journalism Conversation on Social Media","type":"publication"},{"authors":["Jonathan Cable","Glyn Mottershead"],"categories":null,"content":"","date":1520553600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1520553600,"objectID":"924a674ac01dfa2e7edfd9e659b9c6f8","permalink":"/publication/can-i-click-it/","publishdate":"2018-03-09T00:00:00Z","relpermalink":"/publication/can-i-click-it/","section":"publication","summary":"Does the sports media just use clickbait to generate traffic, and is this is reducing the quality of football journalism?","tags":["Sport Journalism","Data Analysis","Social Media","Journalism Studies"],"title":"'Can I click it? Yes you can': Football journalism, Twitter and clickbait","type":"publication"},{"authors":["Glyn Mottershead","Martin Chorley"],"categories":null,"content":"","date":1504306800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504306800,"objectID":"dc6b0ce608ed2e73e6745eb115fa6759","permalink":"/talk/gap_foj/","publishdate":"2017-09-02T00:00:00+01:00","relpermalink":"/talk/gap_foj/","section":"talk","summary":"Fake news, open data, algorithms and news room experimentation mean there's a need to train journalists beyond the basics.","tags":["Journalism Education"],"title":"Recoding journalism  education: mind the  skills gap, please!","type":"talk"},{"authors":["Glyn Mottershead"],"categories":null,"content":"","date":1499814000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499814000,"objectID":"55e70be41ee4b6555ccc370b7430e92d","permalink":"/talk/visualising-on-a-deadline/","publishdate":"2017-07-12T00:00:00+01:00","relpermalink":"/talk/visualising-on-a-deadline/","section":"talk","summary":"How do you deal with statistical uncertainty when you're up against the clock?","tags":["Journalism Education","Computational Journalism","Data Journalism","Data Visualisation"],"title":"The First Commit of History: Visualising News on a Deadline","type":"talk"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1499295600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1499295600,"objectID":"eac104f5060d19d1aedb58c42c608331","permalink":"/talk/now-we-are-three/","publishdate":"2017-07-06T00:00:00+01:00","relpermalink":"/talk/now-we-are-three/","section":"talk","summary":"Key findings from the process of designing a new joint-honours Master’s degree combining Computer Science and Journalism.","tags":["Journalism Education","Computational Journalism","Data Journalism"],"title":"Now we are three: A perspective on Computational and Data Journalism Education","type":"talk"},{"authors":["Martin Chorley","Glyn Mottershead"],"categories":null,"content":"","date":1475362800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1475362800,"objectID":"900514d6a949783fe95035d5f94e4e80","permalink":"/publication/are-you-talking-to-me/","publishdate":"2016-10-02T00:00:00+01:00","relpermalink":"/publication/are-you-talking-to-me/","section":"publication","summary":"This preliminary research aims to show how computational analysis allows us to investigate the means by which news stories are spread.","tags":["Twitter","Data Analysis","Social Media","Journalism Studies"],"title":"Are You Talking To Me?: An Analysis of Journalism Conversation on Social Media","type":"publication"},{"authors":["Tim Holmes","Glyn Mottershead"],"categories":null,"content":"","date":1440716400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1440716400,"objectID":"e4680d23b8ea482b608fd38abc270126","permalink":"/publication/subbing-book/","publishdate":"2015-08-28T00:00:00+01:00","relpermalink":"/publication/subbing-book/","section":"publication","summary":"Subediting and Production for Journalists is an introduction to the skills of subediting for newspapers, magazines and websites.","tags":["Subediting","Newspaper Design","Magazine Design"],"title":"Subediting and production for journalists","type":"publication"},{"authors":["Tim Holmes","Sarah Hadwin","Glyn Mottershead"],"categories":null,"content":"","date":1341097200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1341097200,"objectID":"23a5953b27b1b69ea603f790f046b6e0","permalink":"/publication/21st-century-journalism-handbook/","publishdate":"2012-07-01T00:00:00+01:00","relpermalink":"/publication/21st-century-journalism-handbook/","section":"publication","summary":"The 21st Century Journalism Handbook is a comprehensive guide to the core principles and practices essential to the modern journalist.","tags":["Journalism","Data Journalism","News Writing","Feature Writing","Journalism Education"],"title":"21st Century Journalism Handbook","type":"publication"},{"authors":["Terry King","Emma Duke-Williams","Glyn Mottershead"],"categories":null,"content":"","date":1249081200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1249081200,"objectID":"b26373d526fa394ae13c765109006d4e","permalink":"/publication/learning-and-knowledge-building/","publishdate":"2009-08-01T00:00:00+01:00","relpermalink":"/publication/learning-and-knowledge-building/","section":"publication","summary":"This paper seeks to draw together findings from a number of research studies to discuss the implications for the development of educational practices in Higher Education towards a student experience which is rich in authenticity.","tags":["Twitter","Data Analysis","Social Media","E-Learning","Education"],"title":"Learning and Knowledge Building with Web 2.0 Technologies: Implications for Teacher Education","type":"publication"}]